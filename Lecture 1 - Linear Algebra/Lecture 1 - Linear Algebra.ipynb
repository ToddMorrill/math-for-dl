{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Oz2hC-gzEV63"
   },
   "source": [
    "# Linear Algebra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6mfwOCq0F_Js"
   },
   "source": [
    "## Motivation\n",
    "\n",
    "We begin by examining the objects of linear algebra, namely scalars, vectors, matrices, and tensors, along with their properties. These object underly much of scientific computing and in particular deep learning modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kw1cF0PwasRD",
    "origin_pos": 0
   },
   "source": [
    "\n",
    "\n",
    "## Scalars\n",
    "\n",
    "**Scalars** - We call values consisting of just one numerical quantity *scalars*.\n",
    "\n",
    "For example 0, 1, and 3.14159 are scalars.\n",
    "\n",
    "Scalar variables are denoted by ordinary lower-cased letters (e.g., $x$, $y$, and $z$).\n",
    "\n",
    "The space of all (continuous) *real-valued* scalars is denoted by $\\mathbb{R}$\n",
    "\n",
    "$x \\in \\mathbb{R}$ is a formal way to say that $x$ is a real-valued scalar.\n",
    "\n",
    "A scalar is represented by a tensor with just one element."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gqcaWykfasRH",
    "origin_pos": 2,
    "outputId": "f4919d3b-d926-40a8-d2a7-7ce64c8a7638",
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([5.]), tensor([6.]), tensor([1.5000]), tensor([9.]))"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# binary operations on scalars\n",
    "import torch\n",
    "\n",
    "x = torch.tensor([3.0])\n",
    "y = torch.tensor([2.0])\n",
    "\n",
    "x + y, x * y, x / y, x**y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jNe1SjptasRI",
    "origin_pos": 4
   },
   "source": [
    "## Vectors\n",
    "\n",
    "**Vectors** - You can think of a vector as simply a list of scalar values.\n",
    "\n",
    "In math notation, we will usually denote vectors as bold-faced,\n",
    "lower-cased letters (e.g., $\\mathbf{x}$, $\\mathbf{y}$, and $\\mathbf{z})$.\n",
    "\n",
    "We work with vectors via one-dimensional tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GOHipVbUasRI",
    "origin_pos": 6,
    "outputId": "59398409-5c30-43b9-bc5b-afe385095923",
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2, 3])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sample vector\n",
    "x = torch.arange(4)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n_IURWieasRJ",
    "origin_pos": 8
   },
   "source": [
    "## Indexing\n",
    "\n",
    "We can refer to any element of a vector by using a subscript. For example, we can refer to the $i^\\mathrm{th}$ element of $\\mathbf{x}$ by $x_i$, where $x_i$ is a scalar.\n",
    "\n",
    "In math, a vector $\\mathbf{x}$ can be written as\n",
    "\n",
    "$$\\mathbf{x} =\\begin{bmatrix}x_{1}  \\\\x_{2}  \\\\ \\vdots  \\\\x_{n}\\end{bmatrix},$$\n",
    "\n",
    "where $x_1, \\ldots, x_n$ are elements of the vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KZrip3H1asRJ",
    "origin_pos": 10,
    "outputId": "01ab7172-82ef-493d-aa59-9fa122484c26",
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# index into a vector\n",
    "x[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dP3spUjgasRK",
    "origin_pos": 12
   },
   "source": [
    "### Length, Dimensionality, and Shape\n",
    "\n",
    "In math notation, if we want to say that a vector $\\mathbf{x}$\n",
    "consists of $n$ real-valued scalars, we can express this as $\\mathbf{x} \\in \\mathbb{R}^n$.\n",
    "\n",
    "The length of a vector is commonly called the *dimension* of the vector.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WxgFRIJ-asRK",
    "origin_pos": 14,
    "outputId": "b6f91e35-9241-4c53-92bd-c48a19f03be5",
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# access with builtin python len() function\n",
    "len(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oaKmsZrfasRK",
    "origin_pos": 18,
    "outputId": "ed399ab5-0fc8-40ba-e762-20399eb3a64e",
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can also access its length via the `.shape` attribute.\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_GBPh36rasRK",
    "origin_pos": 20
   },
   "source": [
    "Note that the word \"dimension\" tends to get overloaded in these contexts and this tends to confuse people.\n",
    "\n",
    "To clarify, we use the dimensionality of a *vector* or an *axis*\n",
    "to refer to its length, i.e., the number of elements of a vector or an axis.\n",
    "\n",
    "However, we use the dimensionality of a tensor to refer to the number of axes that a tensor has. In this sense, the dimensionality of some axis of a tensor will be the length of that axis.\n",
    "\n",
    "For example, the dimensionality of the following is 3.\n",
    "$$\\mathbf{x} =\\begin{bmatrix}x_{1}  \\\\x_{2}  \\\\x_{3}\\end{bmatrix},$$\n",
    "\n",
    "But the dimensionality of the following is 2.\n",
    "\n",
    "$$\\mathbf{A}=\\begin{bmatrix} a_{11} & a_{12} & \\cdots & a_{1n} \\\\ a_{21} & a_{22} & \\cdots & a_{2n} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ a_{m1} & a_{m2} & \\cdots & a_{mn} \\\\ \\end{bmatrix}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qpbCtTFTIulS"
   },
   "source": [
    "## Matrices\n",
    "\n",
    "**Matrices** - A matrix is a collection of vectors in $\\mathbb{R}^n$\n",
    "\n",
    "Matrices, are typically denoted with bold-faced, capital letters\n",
    "(e.g., $\\mathbf{X}$, $\\mathbf{Y}$, and $\\mathbf{Z}$).\n",
    "\n",
    "In math notation, we use $\\mathbf{A} \\in \\mathbb{R}^{m \\times n}$\n",
    "to express that the matrix $\\mathbf{A}$ consists of $m$ rows and $n$ columns of real-valued scalars.\n",
    "Visually, we can illustrate any matrix $\\mathbf{A} \\in \\mathbb{R}^{m \\times n}$ as a table,\n",
    "where each element $a_{ij}$ belongs to the $i^{\\mathrm{th}}$ row and $j^{\\mathrm{th}}$ column:\n",
    "\n",
    "$$\\mathbf{A}=\\begin{bmatrix} a_{11} & a_{12} & \\cdots & a_{1n} \\\\ a_{21} & a_{22} & \\cdots & a_{2n} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ a_{m1} & a_{m2} & \\cdots & a_{mn} \\\\ \\end{bmatrix}.$$\n",
    "\n",
    "For any $\\mathbf{A} \\in \\mathbb{R}^{m \\times n}$, the shape of $\\mathbf{A}$ is ($m$, $n$) or $m \\times n$.\n",
    "\n",
    "Specifically, when a matrix has the same number of rows and columns,\n",
    "its shape becomes a square; thus, it is called a **square matrix**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IMTdpbKuasRL",
    "origin_pos": 22,
    "outputId": "a7782991-9fc9-4e1f-e526-baa3d94429ec",
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1,  2,  3],\n",
       "        [ 4,  5,  6,  7],\n",
       "        [ 8,  9, 10, 11],\n",
       "        [12, 13, 14, 15],\n",
       "        [16, 17, 18, 19]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example matrix\n",
    "A = torch.arange(20).reshape(5, 4)\n",
    "A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wUjGkPErasRL",
    "origin_pos": 24
   },
   "source": [
    "## Indexing Matrices\n",
    "We can access the scalar element $a_{ij}$ of a matrix $\\mathbf{A}$ by specifying the indices for the row ($i$) and column ($j$), such as $[\\mathbf{A}]_{ij}$.\n",
    "\n",
    "When the scalar elements of a matrix $\\mathbf{A}$ are not given, we may simply use the lower-case letter of the matrix $\\mathbf{A}$ with the index subscript, $a_{ij}$, to refer to $[\\mathbf{A}]_{ij}$.\n",
    "\n",
    "When we exchange a matrix's rows and columns, the result is called the **transpose** of the matrix.\n",
    "\n",
    "Formally, we signify a matrix $\\mathbf{A}$'s transpose by $\\mathbf{A}^\\top$ and if $\\mathbf{B} = \\mathbf{A}^\\top$, then $b_{ij} = a_{ji}$ for any $i$ and $j$. Thus, the transpose of $\\mathbf{A}$ is a $n \\times m$ matrix:\n",
    "\n",
    "$$\n",
    "\\mathbf{A}^\\top =\n",
    "\\begin{bmatrix}\n",
    "    a_{11} & a_{21} & \\dots  & a_{m1} \\\\\n",
    "    a_{12} & a_{22} & \\dots  & a_{m2} \\\\\n",
    "    \\vdots & \\vdots & \\ddots  & \\vdots \\\\\n",
    "    a_{1n} & a_{2n} & \\dots  & a_{mn}\n",
    "\\end{bmatrix}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Understanding check:** What's the intuitive way to describe the transpose? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "Jj3wonAkasRL",
    "origin_pos": 26,
    "outputId": "9e6da8bc-5697-4371-fa0f-eebb12cc3c6d",
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  4,  8, 12, 16],\n",
       "        [ 1,  5,  9, 13, 17],\n",
       "        [ 2,  6, 10, 14, 18],\n",
       "        [ 3,  7, 11, 15, 19]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# transpose in code\n",
    "A.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IbPYCfQqasRM",
    "origin_pos": 28
   },
   "source": [
    "As a special type of the square matrix, a **symmetric matrix** $\\mathbf{A}$ is equal to its transpose: $\\mathbf{A} = \\mathbf{A}^\\top$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "4f0kIF48asRM",
    "origin_pos": 30,
    "outputId": "b9e0e6f5-f13b-41ef-c5d5-4409d687c45e",
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3],\n",
       "        [2, 0, 4],\n",
       "        [3, 4, 5]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a symmetric matrix B.\n",
    "B = torch.tensor([[1, 2, 3], [2, 0, 4], [3, 4, 5]])\n",
    "B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ofTWCtBCasRM",
    "origin_pos": 32
   },
   "source": [
    "Now we compare `B` with its transpose.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "GOUDrt5vasRM",
    "origin_pos": 34,
    "outputId": "6f1abc72-9982-4e23-e9fa-93b962e8e5c1",
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[True, True, True],\n",
       "        [True, True, True],\n",
       "        [True, True, True]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B == B.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nz9OJw0nasRM",
    "origin_pos": 36
   },
   "source": [
    "Matrices are useful for machine learning!\n",
    "For example, rows in our matrix might correspond to different examples (e.g. houses), while columns might correspond to different attributes (e.g. price, number of bedrooms, etc.)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nz9OJw0nasRM",
    "origin_pos": 36
   },
   "source": [
    "## Tensors\n",
    "\n",
    "**Tensors** - (\"tensors\" in this subsection refer to algebraic objects) give us a generic way of describing $n$-dimensional arrays with an arbitrary number of axes.\n",
    "\n",
    "Tensors are denoted with capital letters of a special font face (e.g., $\\mathsf{X}$, $\\mathsf{Y}$, and $\\mathsf{Z}$) and their indexing mechanism (e.g., $x_{ijk}$ and $[\\mathsf{X}]_{1, 2i-1, 3}$) is similar to that of matrices.\n",
    "\n",
    "**NB:** vectors are first-order tensors and matrices are second-order tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "NvRNIbHBasRN",
    "origin_pos": 38,
    "outputId": "8115cac3-2baf-4226-976f-40b6301f2f8f",
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0,  1,  2,  3],\n",
       "         [ 4,  5,  6,  7],\n",
       "         [ 8,  9, 10, 11]],\n",
       "\n",
       "        [[12, 13, 14, 15],\n",
       "         [16, 17, 18, 19],\n",
       "         [20, 21, 22, 23]]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.arange(24).reshape(2, 3, 4)\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensors are important data structures for all sorts of data such as images, which have 3 axes corresponding to the height, width, and a *channel* for stacking the color channels (red, green, and blue)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fZcGtjibasRN",
    "origin_pos": 40
   },
   "source": [
    "## Basic Properties of Tensor Arithmetic\n",
    "\n",
    "Unary operations and elementwise binary operations maintain the shape of the tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "JwKWhm5pasRN",
    "origin_pos": 42,
    "outputId": "9a80704d-d128-4063-db05-cf81cdeda0dd",
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.,  1.,  2.,  3.],\n",
       "         [ 4.,  5.,  6.,  7.],\n",
       "         [ 8.,  9., 10., 11.],\n",
       "         [12., 13., 14., 15.],\n",
       "         [16., 17., 18., 19.]]),\n",
       " tensor([[ 0.,  2.,  4.,  6.],\n",
       "         [ 8., 10., 12., 14.],\n",
       "         [16., 18., 20., 22.],\n",
       "         [24., 26., 28., 30.],\n",
       "         [32., 34., 36., 38.]]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# elementwise binary operation\n",
    "A = torch.arange(20, dtype=torch.float32).reshape(5, 4)\n",
    "B = A.clone()  # Assign a copy of `A` to `B` by allocating new memory\n",
    "A, A + B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q-jDEQSbasRN",
    "origin_pos": 44
   },
   "source": [
    "Specifically, elementwise multiplication of two matrices is called their *Hadamard product* (math notation $\\odot$).\n",
    "Consider matrix $\\mathbf{B} \\in \\mathbb{R}^{m \\times n}$ whose element of row $i$ and column $j$ is $b_{ij}$. The Hadamard product of matrices $\\mathbf{A}$ and $\\mathbf{B}$ is:\n",
    "\n",
    "$$\n",
    "\\mathbf{A} \\odot \\mathbf{B} =\n",
    "\\begin{bmatrix}\n",
    "    a_{11}  b_{11} & a_{12}  b_{12} & \\dots  & a_{1n}  b_{1n} \\\\\n",
    "    a_{21}  b_{21} & a_{22}  b_{22} & \\dots  & a_{2n}  b_{2n} \\\\\n",
    "    \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    a_{m1}  b_{m1} & a_{m2}  b_{m2} & \\dots  & a_{mn}  b_{mn}\n",
    "\\end{bmatrix}.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "tPByyf7uasRO",
    "origin_pos": 46,
    "outputId": "a875856a-4bd9-47fd-eefc-fbf205d33ac1",
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  0.,   1.,   4.,   9.],\n",
       "        [ 16.,  25.,  36.,  49.],\n",
       "        [ 64.,  81., 100., 121.],\n",
       "        [144., 169., 196., 225.],\n",
       "        [256., 289., 324., 361.]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# elementwise multiplication (aka Hadamard product)\n",
    "A * B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YFLvZWM5asRO",
    "origin_pos": 48
   },
   "source": [
    "Multiplying or adding a tensor by a scalar also does not change the shape of the tensor, where each element of the operand tensor will be added or multiplied by the scalar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "-GNgOgMSasRO",
    "origin_pos": 50,
    "outputId": "cc2c8231-ea0a-45b5-c35c-4f3d282b7ceb",
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 0,  1,  2,  3],\n",
       "          [ 4,  5,  6,  7],\n",
       "          [ 8,  9, 10, 11]],\n",
       " \n",
       "         [[12, 13, 14, 15],\n",
       "          [16, 17, 18, 19],\n",
       "          [20, 21, 22, 23]]]),\n",
       " tensor([[[ 2,  3,  4,  5],\n",
       "          [ 6,  7,  8,  9],\n",
       "          [10, 11, 12, 13]],\n",
       " \n",
       "         [[14, 15, 16, 17],\n",
       "          [18, 19, 20, 21],\n",
       "          [22, 23, 24, 25]]]),\n",
       " torch.Size([2, 3, 4]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# matrix-scalar multiplication\n",
    "a = 2\n",
    "X = torch.arange(24).reshape(2, 3, 4)\n",
    "X, a + X, (a * X).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eWSytmJNasRO",
    "origin_pos": 52
   },
   "source": [
    "## Reduction\n",
    "\n",
    "One useful operation that we can perform with arbitrary tensors is to calculate the sum of their elements.\n",
    "\n",
    "In mathematical notation, we express sums using the $\\sum$ symbol. To express the sum of the elements in a vector $\\mathbf{x}$ of length $d$, we write $\\sum_{i=1}^d x_i$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "7BdLRSZvasRO",
    "origin_pos": 54,
    "outputId": "395bb500-93c9-4fef-a362-d35042a80191",
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0., 1., 2., 3.]), tensor(6.))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sum\n",
    "x = torch.arange(4, dtype=torch.float32)\n",
    "x, x.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qNGWtQSeasRP",
    "origin_pos": 56
   },
   "source": [
    "We can express sums over the elements of tensors of arbitrary shape.\n",
    "\n",
    "For example, the sum of the elements of an $m \\times n$ matrix $\\mathbf{A}$ could be written $\\sum_{i=1}^{m} \\sum_{j=1}^{n} a_{ij}$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "xKny8T8tasRP",
    "origin_pos": 58,
    "outputId": "bb13cc84-21c1-43b3-d25e-1e88ebb39d8b",
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 4]), tensor(190.))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# matrix sum\n",
    "A.shape, A.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3iJ_L5IdasRP",
    "origin_pos": 60
   },
   "source": [
    "By default, invoking the function for calculating the sum *reduces* a tensor along all its axes to a scalar.\n",
    "\n",
    "To reduce the row dimension (axis 0) by summing up elements of all the rows, we specify `axis=0` when invoking the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "bjt-_YWgasRP",
    "origin_pos": 62,
    "outputId": "bc858435-672e-41c6-a3b5-288aca765226",
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.,  1.,  2.,  3.],\n",
       "         [ 4.,  5.,  6.,  7.],\n",
       "         [ 8.,  9., 10., 11.],\n",
       "         [12., 13., 14., 15.],\n",
       "         [16., 17., 18., 19.]]),\n",
       " tensor([40., 45., 50., 55.]),\n",
       " torch.Size([4]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A_sum_axis0 = A.sum(axis=0)\n",
    "A, A_sum_axis0, A_sum_axis0.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iztdnS8VasRP",
    "origin_pos": 64
   },
   "source": [
    "Specifying `axis=1` will reduce the column dimension (axis 1) by summing up elements of all the columns.\n",
    "Thus, the dimension of axis 1 of the input is lost in the output shape.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "8UJs1XpaasRQ",
    "origin_pos": 66,
    "outputId": "969015d6-4929-4e24-b01f-8d2946ec8e10",
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.,  1.,  2.,  3.],\n",
       "         [ 4.,  5.,  6.,  7.],\n",
       "         [ 8.,  9., 10., 11.],\n",
       "         [12., 13., 14., 15.],\n",
       "         [16., 17., 18., 19.]]),\n",
       " tensor([ 6., 22., 38., 54., 70.]),\n",
       " torch.Size([5]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A_sum_axis1 = A.sum(axis=1)\n",
    "A, A_sum_axis1, A_sum_axis1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CsQxN66EasRQ",
    "origin_pos": 68
   },
   "source": [
    "Reducing a matrix along both rows and columns via summation\n",
    "is equivalent to summing up all the elements of the matrix.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "b2DrwYClasRQ",
    "origin_pos": 70,
    "outputId": "4f7a2308-407f-4566-f94d-71bd734c290e",
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(190.)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.sum(axis=[0, 1])  # Same as `A.sum()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N47VQJjoasRQ",
    "origin_pos": 72
   },
   "source": [
    "A related quantity is the **mean**, which is also called the **average**. We calculate the mean by dividing the sum by the total number of elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "_2_iQtniasRQ",
    "origin_pos": 74,
    "outputId": "f0fbb715-89dd-46fa-90c4-016ace578a8e",
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(9.5000), tensor(9.5000))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.mean(), A.sum() / A.numel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7i0JIZppasRQ",
    "origin_pos": 76
   },
   "source": [
    "Likewise, the function for calculating the mean can also reduce a tensor along the specified axes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "OjTUOWseasRQ",
    "origin_pos": 78,
    "outputId": "cca1bdac-2f0e-4590-8336-1fdd09748cc2",
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.,  1.,  2.,  3.],\n",
       "         [ 4.,  5.,  6.,  7.],\n",
       "         [ 8.,  9., 10., 11.],\n",
       "         [12., 13., 14., 15.],\n",
       "         [16., 17., 18., 19.]]),\n",
       " tensor([ 8.,  9., 10., 11.]),\n",
       " tensor([ 8.,  9., 10., 11.]))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A, A.mean(axis=0), A.sum(axis=0) / A.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MydsCk81asRR",
    "origin_pos": 80
   },
   "source": [
    "### Non-Reduction Sum\n",
    "However, sometimes it can be useful to keep the number of axes unchanged\n",
    "when invoking the function for calculating the sum or mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "73qKCppBasRR",
    "origin_pos": 82,
    "outputId": "6c30f00c-fe2d-4d03-bb17-b704599b8aa9",
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.,  1.,  2.,  3.],\n",
       "         [ 4.,  5.,  6.,  7.],\n",
       "         [ 8.,  9., 10., 11.],\n",
       "         [12., 13., 14., 15.],\n",
       "         [16., 17., 18., 19.]]),\n",
       " tensor([[ 6.],\n",
       "         [22.],\n",
       "         [38.],\n",
       "         [54.],\n",
       "         [70.]]),\n",
       " torch.Size([5, 4]),\n",
       " torch.Size([5, 1]))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum_A = A.sum(axis=1, keepdims=True)\n",
    "A, sum_A, A.shape, sum_A.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W-JJePScasRR",
    "origin_pos": 84
   },
   "source": [
    "For instance, since `sum_A` still keeps its two axes after summing each row, we can divide `A` by `sum_A` with broadcasting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "j1uMoMnQasRR",
    "origin_pos": 86,
    "outputId": "6fd2fddf-5168-4d8c-fc7a-4112b97c372d",
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 0.1667, 0.3333, 0.5000],\n",
       "        [0.1818, 0.2273, 0.2727, 0.3182],\n",
       "        [0.2105, 0.2368, 0.2632, 0.2895],\n",
       "        [0.2222, 0.2407, 0.2593, 0.2778],\n",
       "        [0.2286, 0.2429, 0.2571, 0.2714]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A / sum_A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ajOa358AasRR",
    "origin_pos": 88
   },
   "source": [
    "If we want to calculate the cumulative sum of elements of `A` along some axis, say `axis=1` (col by col),\n",
    "we can call the `cumsum` function. This function will not reduce the input tensor along any axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "yjaDNAKHasRR",
    "origin_pos": 90,
    "outputId": "47844604-e98e-471f-fbcb-c09c76b77fb8",
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.,  1.,  2.,  3.],\n",
       "         [ 4.,  5.,  6.,  7.],\n",
       "         [ 8.,  9., 10., 11.],\n",
       "         [12., 13., 14., 15.],\n",
       "         [16., 17., 18., 19.]]),\n",
       " tensor([[ 0.,  1.,  3.,  6.],\n",
       "         [ 4.,  9., 15., 22.],\n",
       "         [ 8., 17., 27., 38.],\n",
       "         [12., 25., 39., 54.],\n",
       "         [16., 33., 51., 70.]]))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A,A.cumsum(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o5vSJrnKasRR",
    "origin_pos": 92
   },
   "source": [
    "## Dot Products\n",
    "\n",
    "**Dot product** - Given two vectors $\\mathbf{x}, \\mathbf{y} \\in \\mathbb{R}^d$, their **dot product** $\\mathbf{x}^\\top \\mathbf{y}$ (or $\\langle \\mathbf{x}, \\mathbf{y}  \\rangle$) is a sum over the products of the elements at the same position: $\\mathbf{x}^\\top \\mathbf{y} = \\sum_{i=1}^{d} x_i y_i$.\n",
    "\n",
    "**Understanding check:** Why do we take the transpose of vector $\\mathbf{x}$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "lEueaFGhasRR",
    "origin_pos": 94,
    "outputId": "0fcba5ff-6378-4a0b-e55f-028d15b0e977",
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0., 1., 2., 3.]), tensor([1., 1., 1., 1.]), tensor(6.))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = torch.ones(4, dtype = torch.float32)\n",
    "x, y, torch.dot(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dp31Hn6-asRS",
    "origin_pos": 96
   },
   "source": [
    "Note that we can express the dot product of two vectors equivalently by performing an elementwise multiplication and then a sum:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "2cnLdeh9asRS",
    "origin_pos": 98,
    "outputId": "f4f2ad42-6492-4174-8c23-4e4f234a5122",
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6.)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(x * y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A7bqtH3UasRS",
    "origin_pos": 100
   },
   "source": [
    "Dot products are useful in a wide range of contexts. For example, given some set of values,\n",
    "denoted by a vector $\\mathbf{x}  \\in \\mathbb{R}^d$ and a set of weights denoted by $\\mathbf{w} \\in \\mathbb{R}^d$,\n",
    "the weighted sum of the values in $\\mathbf{x}$ according to the weights $\\mathbf{w}$ could be expressed as the dot product $\\mathbf{x}^\\top \\mathbf{w}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A7bqtH3UasRS",
    "origin_pos": 100
   },
   "source": [
    "## Matrix-Vector Products\n",
    "\n",
    "**Matrix-vector products** - Recall the matrix $\\mathbf{A} \\in \\mathbb{R}^{m \\times n}$\n",
    "and the vector $\\mathbf{x} \\in \\mathbb{R}^n$. The matrix-vector product $\\mathbf{A}\\mathbf{x}$\n",
    "is simply a column vector of length $m$, whose $i^\\mathrm{th}$ element is the dot product $\\mathbf{a}^\\top_i \\mathbf{x}$\n",
    "\n",
    "To make this more clear let us visualize the matrix $\\mathbf{A}$ in terms of its row vectors:\n",
    "\n",
    "$$\\mathbf{A}=\n",
    "\\begin{bmatrix}\n",
    "\\mathbf{a}^\\top_{1} \\\\\n",
    "\\mathbf{a}^\\top_{2} \\\\\n",
    "\\vdots \\\\\n",
    "\\mathbf{a}^\\top_m \\\\\n",
    "\\end{bmatrix},$$\n",
    "\n",
    "where each $\\mathbf{a}^\\top_{i} \\in \\mathbb{R}^n$ is a row vector representing the $i^\\mathrm{th}$ row of the matrix $\\mathbf{A}$.\n",
    "\n",
    "Now the matrix-vector product $\\mathbf{A}\\mathbf{x}$ is represented as:\n",
    "\n",
    "$$\n",
    "\\mathbf{A}\\mathbf{x}\n",
    "= \\begin{bmatrix}\n",
    "\\mathbf{a}^\\top_{1} \\\\\n",
    "\\mathbf{a}^\\top_{2} \\\\\n",
    "\\vdots \\\\\n",
    "\\mathbf{a}^\\top_m \\\\\n",
    "\\end{bmatrix}\\mathbf{x}\n",
    "= \\begin{bmatrix}\n",
    " \\mathbf{a}^\\top_{1} \\mathbf{x}  \\\\\n",
    " \\mathbf{a}^\\top_{2} \\mathbf{x} \\\\\n",
    "\\vdots\\\\\n",
    " \\mathbf{a}^\\top_{m} \\mathbf{x}\\\\\n",
    "\\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "We can think of multiplication by a matrix $\\mathbf{A}\\in \\mathbb{R}^{m \\times n}$\n",
    "as a transformation that projects vectors\n",
    "from $\\mathbb{R}^{n}$ to $\\mathbb{R}^{m}$.\n",
    "\n",
    "Matrix-vector products are heavily used in a neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.,  1.,  2.,  3.],\n",
       "         [ 4.,  5.,  6.,  7.],\n",
       "         [ 8.,  9., 10., 11.],\n",
       "         [12., 13., 14., 15.],\n",
       "         [16., 17., 18., 19.]]),\n",
       " tensor([0., 1., 2., 3.]))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A, x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "imsljLDdasRS",
    "origin_pos": 102,
    "outputId": "88a6940b-db30-46c4-80b8-ae8dcab50fc0",
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 4]), torch.Size([4]), tensor([ 14.,  38.,  62.,  86., 110.]))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.shape, x.shape, torch.mv(A, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Understanding check:** What's the difference between `torch.mv` and `torch.mm`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.,  1.,  2.,  3.],\n",
       "         [ 4.,  5.,  6.,  7.],\n",
       "         [ 8.,  9., 10., 11.],\n",
       "         [12., 13., 14., 15.],\n",
       "         [16., 17., 18., 19.]]),\n",
       " tensor([[0.],\n",
       "         [1.],\n",
       "         [2.],\n",
       "         [3.]]),\n",
       " torch.Size([5, 4]),\n",
       " torch.Size([4, 1]))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A, x.unsqueeze(-1), A.shape, x.unsqueeze(-1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# throws error because x is no longer a vector\n",
    "# torch.mv(A, x.unsqueeze(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 14.],\n",
       "        [ 38.],\n",
       "        [ 62.],\n",
       "        [ 86.],\n",
       "        [110.]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# but this works\n",
    "torch.mm(A, x.unsqueeze(-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pa0VsmXzasRS",
    "origin_pos": 104
   },
   "source": [
    "## Matrix-Matrix Multiplication\n",
    "\n",
    "**Matrix-matrix multiplication** - Say that we have two matrices $\\mathbf{A} \\in \\mathbb{R}^{n \\times k}$ and $\\mathbf{B} \\in \\mathbb{R}^{k \\times m}$ and their matrix product is denoted $\\mathbf{C} = \\mathbf{A}\\mathbf{B}$ then the matrix product $\\mathbf{C} \\in \\mathbb{R}^{n \\times m}$ contains elements $c_{ij}$ as the dot product $\\mathbf{a}^\\top_i \\mathbf{b}_j$.\n",
    "\n",
    "$$\\mathbf{A}=\\begin{bmatrix}\n",
    " a_{11} & a_{12} & \\cdots & a_{1k} \\\\\n",
    " a_{21} & a_{22} & \\cdots & a_{2k} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    " a_{n1} & a_{n2} & \\cdots & a_{nk} \\\\\n",
    "\\end{bmatrix},\\quad\n",
    "\\mathbf{B}=\\begin{bmatrix}\n",
    " b_{11} & b_{12} & \\cdots & b_{1m} \\\\\n",
    " b_{21} & b_{22} & \\cdots & b_{2m} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    " b_{k1} & b_{k2} & \\cdots & b_{km} \\\\\n",
    "\\end{bmatrix}.$$\n",
    "\n",
    "\n",
    "Denote by $\\mathbf{a}^\\top_{i} \\in \\mathbb{R}^k$\n",
    "the row vector representing the $i^\\mathrm{th}$ row of the matrix $\\mathbf{A}$,\n",
    "and let $\\mathbf{b}_{j} \\in \\mathbb{R}^k$\n",
    "be the column vector from the $j^\\mathrm{th}$ column of the matrix $\\mathbf{B}$.\n",
    "To produce the matrix product $\\mathbf{C} = \\mathbf{A}\\mathbf{B}$, it is easiest to think of $\\mathbf{A}$ in terms of its row vectors and $\\mathbf{B}$ in terms of its column vectors:\n",
    "\n",
    "$$\\mathbf{A}=\n",
    "\\begin{bmatrix}\n",
    "\\mathbf{a}^\\top_{1} \\\\\n",
    "\\mathbf{a}^\\top_{2} \\\\\n",
    "\\vdots \\\\\n",
    "\\mathbf{a}^\\top_n \\\\\n",
    "\\end{bmatrix},\n",
    "\\quad \\mathbf{B}=\\begin{bmatrix}\n",
    " \\mathbf{b}_{1} & \\mathbf{b}_{2} & \\cdots & \\mathbf{b}_{m} \\\\\n",
    "\\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "$$\\mathbf{C} = \\mathbf{AB} = \\begin{bmatrix}\n",
    "\\mathbf{a}^\\top_{1} \\\\\n",
    "\\mathbf{a}^\\top_{2} \\\\\n",
    "\\vdots \\\\\n",
    "\\mathbf{a}^\\top_n \\\\\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    " \\mathbf{b}_{1} & \\mathbf{b}_{2} & \\cdots & \\mathbf{b}_{m} \\\\\n",
    "\\end{bmatrix}\n",
    "= \\begin{bmatrix}\n",
    "\\mathbf{a}^\\top_{1} \\mathbf{b}_1 & \\mathbf{a}^\\top_{1}\\mathbf{b}_2& \\cdots & \\mathbf{a}^\\top_{1} \\mathbf{b}_m \\\\\n",
    " \\mathbf{a}^\\top_{2}\\mathbf{b}_1 & \\mathbf{a}^\\top_{2} \\mathbf{b}_2 & \\cdots & \\mathbf{a}^\\top_{2} \\mathbf{b}_m \\\\\n",
    " \\vdots & \\vdots & \\ddots &\\vdots\\\\\n",
    "\\mathbf{a}^\\top_{n} \\mathbf{b}_1 & \\mathbf{a}^\\top_{n}\\mathbf{b}_2& \\cdots& \\mathbf{a}^\\top_{n} \\mathbf{b}_m\n",
    "\\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "We can think of the matrix-matrix multiplication $\\mathbf{AB}$ as simply performing $m$ matrix-vector products and stitching the results together to form an $n \\times m$ matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "tUYJIpx-asRS",
    "origin_pos": 106,
    "outputId": "979db5f8-0059-4556-ade2-735215733a01",
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.,  1.,  2.,  3.],\n",
       "         [ 4.,  5.,  6.,  7.],\n",
       "         [ 8.,  9., 10., 11.],\n",
       "         [12., 13., 14., 15.],\n",
       "         [16., 17., 18., 19.]]),\n",
       " tensor([[1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.]]),\n",
       " tensor([[ 6.,  6.,  6.],\n",
       "         [22., 22., 22.],\n",
       "         [38., 38., 38.],\n",
       "         [54., 54., 54.],\n",
       "         [70., 70., 70.]]))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example\n",
    "B = torch.ones(4, 3)\n",
    "A, B, torch.mm(A, B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "POgN74QQasRT",
    "origin_pos": 108
   },
   "source": [
    "Matrix-matrix multiplication can be simply called *matrix multiplication*, and should not be confused with the Hadamard product."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Understanding check:** What's the difference between `torch.mm` and `torch.matmul`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.,  1.,  2.,  3.],\n",
       "         [ 4.,  5.,  6.,  7.],\n",
       "         [ 8.,  9., 10., 11.],\n",
       "         [12., 13., 14., 15.],\n",
       "         [16., 17., 18., 19.]]),\n",
       " tensor([[[1.],\n",
       "          [1.],\n",
       "          [1.]],\n",
       " \n",
       "         [[1.],\n",
       "          [1.],\n",
       "          [1.]],\n",
       " \n",
       "         [[1.],\n",
       "          [1.],\n",
       "          [1.]],\n",
       " \n",
       "         [[1.],\n",
       "          [1.],\n",
       "          [1.]]]),\n",
       " torch.Size([5, 4]),\n",
       " torch.Size([4, 3, 1]))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A, B.unsqueeze(-1), A.shape, B.unsqueeze(-1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# throws error because x is no longer a vector\n",
    "# torch.mm(A, B.unsqueeze(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 1, 4]),\n",
       " torch.Size([4, 3]),\n",
       " tensor([[[ 6.,  6.,  6.]],\n",
       " \n",
       "         [[22., 22., 22.]],\n",
       " \n",
       "         [[38., 38., 38.]],\n",
       " \n",
       "         [[54., 54., 54.]],\n",
       " \n",
       "         [[70., 70., 70.]]]),\n",
       " torch.Size([5, 1, 3]))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# matmul works for higher-dimension tensors\n",
    "A.unsqueeze(1).shape, B.shape, torch.matmul(A.unsqueeze(1), B), torch.matmul(A.unsqueeze(1), B).shape "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "POgN74QQasRT",
    "origin_pos": 108
   },
   "source": [
    "## Norms\n",
    "\n",
    "**Norm** - Informally, the norm of a vector tells us how *big* a vector is.\n",
    "\n",
    "The notion of *size* under consideration here concerns not dimensionality but rather the magnitude of the components.\n",
    "\n",
    "In linear algebra, a vector norm is a function $f$ that maps a vector to a scalar, satisfying a handful of properties.\n",
    "\n",
    "Given any vector $\\mathbf{x}$, the first property says that if we scale all the elements of a vector\n",
    "by a constant factor $\\alpha$, its norm also scales by the *absolute value* of the same constant factor:\n",
    "\n",
    "$$f(\\alpha \\mathbf{x}) = |\\alpha| f(\\mathbf{x}).$$\n",
    "\n",
    "\n",
    "The second property is the familiar triangle inequality:\n",
    "\n",
    "$$f(\\mathbf{x} + \\mathbf{y}) \\leq f(\\mathbf{x}) + f(\\mathbf{y}).$$\n",
    "\n",
    "\n",
    "The third property simply says that the norm must be non-negative:\n",
    "\n",
    "$$f(\\mathbf{x}) \\geq 0.$$\n",
    "\n",
    "The final property requires that the smallest norm is achieved and only achieved\n",
    "by a vector consisting of all zeros.\n",
    "\n",
    "$$\\forall i, [\\mathbf{x}]_i = 0 \\Leftrightarrow f(\\mathbf{x})=0.$$\n",
    "\n",
    "The Euclidean distance is a norm: specifically it is the $L_2$ norm.\n",
    "\n",
    "Suppose that the elements in the $n$-dimensional vector\n",
    "$\\mathbf{x}$ are $x_1, \\ldots, x_n$.\n",
    "The $L_2$ *norm* of $\\mathbf{x}$ is the square root of the sum of the squares of the vector elements:\n",
    "\n",
    "$$\\|\\mathbf{x}\\|_2 = \\sqrt{\\sum_{i=1}^n x_i^2},$$\n",
    "\n",
    "where the subscript $2$ is often omitted in $L_2$ norms, i.e., $\\|\\mathbf{x}\\|$ is equivalent to $\\|\\mathbf{x}\\|_2$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "qoElRDhXasRT",
    "origin_pos": 110,
    "outputId": "81f15878-3ed0-4b06-c0c9-214ef640041b",
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5.)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u = torch.tensor([3.0, -4.0])\n",
    "torch.norm(u)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fz5wp7XZasRT",
    "origin_pos": 112
   },
   "source": [
    "In deep learning, we work more often\n",
    "with the squared $L_2$ norm.\n",
    "You will also frequently encounter the $L_1$ *norm*,\n",
    "which is expressed as the sum of the absolute values of the vector elements:\n",
    "\n",
    "$$\\|\\mathbf{x}\\|_1 = \\sum_{i=1}^n \\left|x_i \\right|.$$\n",
    "\n",
    "As compared with the $L_2$ norm,\n",
    "it is less influenced by outliers.\n",
    "To calculate the $L_1$ norm, we compose\n",
    "the absolute value function with a sum over the elements.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "1OZmTRCGasRT",
    "origin_pos": 114,
    "outputId": "8b7764bf-1237-42c1-9f9f-597a3a3fce61",
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(7.)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.abs(u).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nz7pcQG_asRT",
    "origin_pos": 116
   },
   "source": [
    "Both the $L_2$ norm and the $L_1$ norm\n",
    "are special cases of the more general $L_p$ *norm*:\n",
    "\n",
    "$$\\|\\mathbf{x}\\|_p = \\left(\\sum_{i=1}^n \\left|x_i \\right|^p \\right)^{1/p}.$$\n",
    "\n",
    "Analogous to $L_2$ norms of vectors,\n",
    "the *Frobenius norm* of a matrix $\\mathbf{X} \\in \\mathbb{R}^{m \\times n}$\n",
    "is the square root of the sum of the squares of the matrix elements:\n",
    "\n",
    "$$\\|\\mathbf{X}\\|_F = \\sqrt{\\sum_{i=1}^m \\sum_{j=1}^n x_{ij}^2}.$$\n",
    "\n",
    "The Frobenius norm satisfies all the properties of vector norms.\n",
    "It behaves as if it were an $L_2$ norm of a matrix-shaped vector.\n",
    "Invoking the following function will calculate the Frobenius norm of a matrix.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "buEigAX2asRT",
    "origin_pos": 118,
    "outputId": "af97a1b9-4251-4203-e383-41f672a95118",
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6.)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.norm(torch.ones((4, 9)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hCXeXOBmasRT",
    "origin_pos": 120
   },
   "source": [
    "### Norms and Objectives\n",
    "In deep learning, we are often trying to solve optimization problems such as *minimizing* the distance between predictions and the ground-truth observations.\n",
    "\n",
    "Oftentimes, the objectives, perhaps the most important components of deep learning algorithms (besides the data), are expressed as norms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hCXeXOBmasRT",
    "origin_pos": 120
   },
   "source": [
    "## Exercises\n",
    "\n",
    "1. Prove that the transpose of a matrix $\\mathbf{A}$'s transpose is $\\mathbf{A}$: $(\\mathbf{A}^\\top)^\\top = \\mathbf{A}$.\n",
    "1. Given two matrices $\\mathbf{A}$ and $\\mathbf{B}$, show that the sum of transposes is equal to the transpose of a sum: $\\mathbf{A}^\\top + \\mathbf{B}^\\top = (\\mathbf{A} + \\mathbf{B})^\\top$.\n",
    "1. Given any square matrix $\\mathbf{A}$, is $\\mathbf{A} + \\mathbf{A}^\\top$ always symmetric? Why?\n",
    "1. We defined the tensor `X` of shape (2, 3, 4) in this section. What is the output of `len(X)`?\n",
    "1. For a tensor `X` of arbitrary shape, does `len(X)` always correspond to the length of a certain axis of `X`? What is that axis?\n",
    "1. Run `A / A.sum(axis=1)` and see what happens. Can you analyze the reason?\n",
    "1. When traveling between two points in Manhattan, what is the distance that you need to cover in terms of the coordinates, i.e., in terms of avenues and streets? Can you travel diagonally?\n",
    "1. Consider a tensor with shape (2, 3, 4). What are the shapes of the summation outputs along axis 0, 1, and 2?\n",
    "1. Feed a tensor with 3 or more axes to the `linalg.norm` function and observe its output. What does this function compute for tensors of arbitrary shape?\n",
    "\n",
    "## Bonus exercise\n",
    "1. Prove that the $L_2$ function satisfies the norm properties."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xQzwuUIt6r87"
   },
   "source": [
    "## Exercise solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Prove that the transpose of a matrix $\\mathbf{A}$'s transpose is $\\mathbf{A}$: $(\\mathbf{A}^\\top)^\\top = \\mathbf{A}$.\n",
    "\n",
    "**Intuition:** refer to the definition of a transpose and check that the claim holds. Big idea: double transpose returns the same result.\n",
    "\n",
    "Let $A' = A^\\top$, which implies $a'_{ij} = a_{ji}$ by the definition of the transpose. Further, let $A'' = A'^\\top$, which implies $a''_{ji} = a'_{ij}$. But this means that $a_{ji} = a'_{ij} = a''_{ji}$, which means all the elements of $A = (\\mathbf{A}^\\top)^\\top$, which shows that the claim holds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Given two matrices $\\mathbf{A}$ and $\\mathbf{B}$, show that the sum of transposes is equal to the transpose of a sum: $\\mathbf{A}^\\top + \\mathbf{B}^\\top = (\\mathbf{A} + \\mathbf{B})^\\top$.\n",
    "\n",
    "**Intuition:** refer to the definition of a transpose and check that the relation holds. Big idea: transpose distributes.\n",
    "\n",
    "Let $A' = A^\\top$, which implies $a'_{ij} = a_{ji}$ by the definition of the transpose. Further, let $B' = B^\\top$, which implies $b'_{ij} = b_{ji}$.\n",
    "\n",
    "Now let $C = \\mathbf{A}^\\top + \\mathbf{B}^\\top$, which implies that $c_{ij} = a'_{ij} + b'_{ij} = a_{ji} + b_{ji}$.\n",
    "\n",
    "Let $C' = (\\mathbf{A} + \\mathbf{B})^\\top$, which implies $c'_{ij} = a_{ji} + b_{ji}$.\n",
    "\n",
    "But this means that $c_{ij} = a_{ji} + b_{ji} = c'_{ij}$, which means all the elements of $C = C'$, which shows that the claim holds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Given any square matrix $\\mathbf{A}$, is $\\mathbf{A} + \\mathbf{A}^\\top$ always symmetric? Why?\n",
    "\n",
    "**Intuition:** Refer to the definition of symmetric and determine if the claim holds. Try translating the definition into the context of this problem as follows: $(\\mathbf{A} + \\mathbf{A}^\\top)^\\top \\stackrel{?}{=}\\mathbf{A} + \\mathbf{A}^\\top$. Another way of thinking about this is to build on the tools from 1) and 2).\n",
    "\n",
    "**Approach one:**\n",
    "\n",
    "Suppose $\\mathbf{A} \\in \\mathbb{R}^{n \\times n}$. Let $\\mathbf{B} = \\mathbf{A} + \\mathbf{A}^\\top$, which implies $b_{ij} = a_{ij} + a_{ji}$. Further, let $\\mathbf{B}' = \\mathbf{B}^\\top$, which implies $b'_{ij} = b_{ji}$. We now show that $b'_{ij} = b_{ij}$. $b'_{ij} = b_{ji} = a_{ji} + a_{ij}$ and $b_{ij} = a_{ij} + a_{ji}$, which shows that that $b'_{ij} = a_{ji} + a_{ij} = a_{ij} + a_{ji} = b_{ij}$ by commutativity of addition.\n",
    "\n",
    "From this we conclude that all element of $\\mathbf{B} = \\mathbf{B}^\\top$ and claim holds.\n",
    "\n",
    "**Approach two:**\n",
    "\n",
    "We know by problem 1) and 2) that $(\\mathbf{A} + \\mathbf{A}^\\top)^\\top = \\mathbf{A}^\\top + (\\mathbf{A}^\\top)^\\top = \\mathbf{A}^\\top + \\mathbf{A}$, which shows that the claim holds.\n",
    "\n",
    "**Example:**\n",
    "$\\mathbf{A} + \\mathbf{A}^\\top = \\mathbf{A'}$ is symmetric. This is because we're pairing numbers across the diagonal and addition is commutative. This wouldn't hold if the matrix was not square because you wouldn't be able to pair all elements of the matrix across the diagonal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example supporting problem 3\n",
    "A = torch.tensor([[1, 2, 3], \n",
    "                  [4, 5, 6],\n",
    "                  [7, 8, 9]])\n",
    "B = A + A.T\n",
    "(B.T == B).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4)\n",
    "X = torch.arange(24).reshape(2,3,4)\n",
    "len(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5) Yes - the first dimension (i.e. the zeroth dimension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 0.0222, 0.0400, 0.0545],\n",
       "        [0.1000, 0.1111, 0.1200, 0.1273],\n",
       "        [0.2000, 0.2000, 0.2000, 0.2000],\n",
       "        [0.3000, 0.2889, 0.2800, 0.2727],\n",
       "        [0.4000, 0.3778, 0.3600, 0.3455]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 6) The issue with A / A.sum(axis=1) is that we are dividing a matrix with 4 columns by 5 values (the reduced sum). \n",
    "# This can be fixed by summing row-wise (i.e. A.sum(axis=0)) and dividing so that the dimensions align. \n",
    "# This is due to broadcasting semantics. https://pytorch.org/docs/stable/notes/broadcasting.html\n",
    "A = torch.arange(20).reshape(5, 4)\n",
    "A / A.sum(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7) This is the $L_1$ norm (aka the Manhattan distance)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 4]), torch.Size([2, 4]), torch.Size([2, 3]))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 8) try drawing this out to visualize the changes!\n",
    "X = torch.arange(24).reshape(2,3,4)\n",
    "X.sum(axis=0).shape, X.sum(axis=1).shape, X.sum(axis=2).shape, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(65.7571, dtype=torch.float64), 65.75712889109438)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 9) This computes a generalized Frobenius norm and takes the $L_2$ norm of all elements in the tensor.\n",
    "X = torch.arange(24, dtype=float).reshape(2, 3, 4)\n",
    "result = torch.norm(X)\n",
    "\n",
    "# sanity check\n",
    "squared_sums = 0\n",
    "for i in range(24):\n",
    "    squared_sums += i**2\n",
    "check = squared_sums**.5\n",
    "\n",
    "result, check"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "Nz9OJw0nasRM",
    "fZcGtjibasRN",
    "eWSytmJNasRO",
    "MydsCk81asRR",
    "o5vSJrnKasRR",
    "A7bqtH3UasRS",
    "Pa0VsmXzasRS"
   ],
   "name": "linear-algebra-lecture.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
